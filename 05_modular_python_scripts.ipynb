{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOM36nvEKk4RmBwUPBV9gaX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## In this notebook, we'll turn the most useful code cells from notebook 04_custom_datasets into a series of Python scripts saved to a directory called going_modular."
      ],
      "metadata": {
        "id": "0y5YfG1z_hof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Create the 'going_modular' directory if it doesn't exist\n",
        "Path(\"going_modular\").mkdir(exist_ok=True)"
      ],
      "metadata": {
        "id": "V-crpnhUWDZM",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114670762,
          "user_tz": 360,
          "elapsed": 44,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        }
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6EUiJXK9DDOP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114670771,
          "user_tz": 360,
          "elapsed": 20,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "5bcc746a-bcc7-4ae9-91e3-ff2f066e8186"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/get_data.py\n"
          ]
        }
      ],
      "source": [
        "# get_data.py\n",
        "\n",
        "%%writefile going_modular/get_data.py\n",
        "\"\"\"\n",
        "Contains functionality for creating data folders and\n",
        "downloading data.\n",
        "\"\"\"\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Set up path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it...\n",
        "if image_path.is_dir():\n",
        "  print(f\"{image_path} directory exists.\")\n",
        "else:\n",
        "  print(f\"Did not find {image_path} directory, creating one...\")\n",
        "  image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  # Download images\n",
        "  with open(data_path / \"pizza_steak_sushi_20_percent.zip\", \"wb\") as f:\n",
        "    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\")\n",
        "    print(\"Downloading pizza, steak, sushi data...\")\n",
        "    f.write(request.content)\n",
        "\n",
        "  # Unzip image data\n",
        "  with zipfile.ZipFile(data_path / \"pizza_steak_sushi_20_percent.zip\", \"r\") as zip_ref:\n",
        "    print(\"Unzipping pizza, steak, sushi data...\")\n",
        "    zip_ref.extractall(image_path)\n",
        "\n",
        "  # Remove zip file\n",
        "  os.remove(data_path / \"pizza_steak_sushi_20_percent.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get images\n",
        "!python going_modular/get_data.py"
      ],
      "metadata": {
        "id": "zERgfIp5JvjK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114674772,
          "user_tz": 360,
          "elapsed": 4006,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "9d17bb26-06d4-4e0b-da82-90e1a5d6e1a5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Did not find data/pizza_steak_sushi directory, creating one...\n",
            "Downloading pizza, steak, sushi data...\n",
            "Unzipping pizza, steak, sushi data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data_setup.py\n",
        "\n",
        "%%writefile going_modular/data_setup.py\n",
        "\"\"\"\n",
        "Contains functionality for creating PyTorch DataLoaders for\n",
        "image classification data.\n",
        "\"\"\"\n",
        "import os\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "def create_dataloaders(\n",
        "    train_dir: str,\n",
        "    test_dir: str,\n",
        "    transform: transforms.Compose,\n",
        "    batch_size: int,\n",
        "    num_workers: int=NUM_WORKERS\n",
        "):\n",
        "  \"\"\"Creates training and testing DataLoaders.\n",
        "\n",
        "  Takes in a training directory and testing directory path and turns\n",
        "  them into PyTorch Datasets and then into PyTorch DataLoaders.\n",
        "\n",
        "  Args:\n",
        "    train_dir: Path to training directory.\n",
        "    test_dir: Path to testing directory.\n",
        "    transform: torchvision transforms to perform on training and testing data.\n",
        "    batch_size: Number of samples per batch in each of the DataLoaders.\n",
        "    num_workers: An integer for number of workers per DataLoader.\n",
        "\n",
        "  Returns:\n",
        "    A tuple of (train_dataloader, test_dataloader, class_names).\n",
        "    Where class_names is a list of the target classes.\n",
        "    Example usage:\n",
        "      train_dataloader, test_dataloader, class_names = \\\n",
        "        = create_dataloaders(train_dir=path/to/train_dir,\n",
        "                             test_dir=path/to/test_dir,\n",
        "                             transform=some_transform,\n",
        "                             batch_size=32,\n",
        "                             num_workers=4)\n",
        "  \"\"\"\n",
        "  # Use ImageFolder to create datasets\n",
        "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
        "\n",
        "  test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
        "\n",
        "  # Get class names\n",
        "  class_names = train_data.classes\n",
        "\n",
        "  # Turn images into data loaders\n",
        "  train_dataloader = DataLoader(\n",
        "      train_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "  test_dataloader = DataLoader(\n",
        "      test_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,  # don't need to shuffle test data\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "\n",
        "  return train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "id": "g7B6BZayHHYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114674797,
          "user_tz": 360,
          "elapsed": 14,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "f3b3de71-29d6-4114-f5b0-f29db2e315b4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/data_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "vfLTflPPJ85K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114674916,
          "user_tz": 360,
          "elapsed": 114,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "6a02d71f-e77c-4bdb-bf57-b90a8d44e6b5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data  going_modular  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = Path(\"data/pizza_steak_sushi\")\n",
        "train_data_path = image_path / \"train\"\n",
        "test_data_path = image_path / \"test\"\n",
        "\n",
        "from torchvision import transforms\n",
        "data_transform_flip = transforms.Compose([\n",
        "    transforms.Resize((224,224)),#64,64)),\n",
        "    # Flip images randomly on the horizontal\n",
        "    transforms.RandomHorizontalFlip(p=0.5),  # p = probability of flip, 0.5 = 50% chance\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# If we'd like to make DataLoader's we can now use the function within data_setup.py like so:\n",
        "\n",
        "from going_modular import data_setup  # Import data_setup.py\n",
        "\n",
        "# Create train/test dataloader and get class names as a list\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_data_path,\n",
        "    test_data_path,\n",
        "    data_transform_flip,\n",
        "    batch_size=1\n",
        ")\n",
        "#    num_workers=2)  # os.cpu_count(), for number of workers per DataLoader\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "id": "gnq1SCRkKESJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114685989,
          "user_tz": 360,
          "elapsed": 11071,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "f88ff8bd-004c-4319-9244-c6d482827bad"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x79342951b390>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x79342951b3d0>,\n",
              " ['pizza', 'steak', 'sushi'])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model_builder.py\n",
        "\n",
        "%%writefile going_modular/model_builder.py\n",
        "\"\"\"\n",
        "Contains PyTorch model code to instantiate a TinyVGG model.\n",
        "\"\"\"\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class TinyVGG(nn.Module):\n",
        "  \"\"\"Creates the TinyVGG architecture.\n",
        "\n",
        "  Replicates the TinyVGG architecture from the CNN explainer website in PyTorch.\n",
        "  See the original architecture here: https://poloclub.github.io/cnn-explainer/\n",
        "\n",
        "  Args:\n",
        "    input_shape: An integer indicating number of input channels.\n",
        "    hidden_units: An integer indicating number of hidden units between layers.\n",
        "    output_shape: An integer indicating number of output units.\n",
        "  \"\"\"\n",
        "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
        "    super().__init__()\n",
        "    self.conv_block_1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=input_shape,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,\n",
        "                     stride=2)\n",
        "    )\n",
        "    self.conv_block_2 = nn.Sequential(\n",
        "        nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2)\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        # Where did this in_features shape come from?\n",
        "        # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
        "        nn.Linear(in_features=hidden_units*56*56, #*13*13,  # =flattened_size\n",
        "                  out_features=output_shape)\n",
        "    )\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    x = self.conv_block_1(x)\n",
        "    x = self.conv_block_2(x)\n",
        "    x = self.classifier(x)\n",
        "    return x\n",
        "    # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # <- leverage the benefits of operator fusion"
      ],
      "metadata": {
        "id": "jG6vbWjgLdtw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114686003,
          "user_tz": 360,
          "elapsed": 11,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "16ecf0c8-4406-4297-b1c3-af9567af4c41"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/model_builder.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls going_modular/"
      ],
      "metadata": {
        "id": "oPXTjCg9NpWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114686096,
          "user_tz": 360,
          "elapsed": 91,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "9943b216-8677-4a40-9c5f-040cbfd97444"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_setup.py  get_data.py  model_builder.py  __pycache__\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat going_modular/model_builder.py"
      ],
      "metadata": {
        "id": "NEJGHtKHn2fW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114686457,
          "user_tz": 360,
          "elapsed": 351,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "842ebb1d-1b29-4f10-f66e-7fd1596dcd97"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"\"\"\n",
            "Contains PyTorch model code to instantiate a TinyVGG model.\n",
            "\"\"\"\n",
            "import torch\n",
            "from torch import nn\n",
            "\n",
            "class TinyVGG(nn.Module):\n",
            "  \"\"\"Creates the TinyVGG architecture.\n",
            "\n",
            "  Replicates the TinyVGG architecture from the CNN explainer website in PyTorch.\n",
            "  See the original architecture here: https://poloclub.github.io/cnn-explainer/\n",
            "\n",
            "  Args:\n",
            "    input_shape: An integer indicating number of input channels.\n",
            "    hidden_units: An integer indicating number of hidden units between layers.\n",
            "    output_shape: An integer indicating number of output units.\n",
            "  \"\"\"\n",
            "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
            "    super().__init__()\n",
            "    self.conv_block_1 = nn.Sequential(\n",
            "        nn.Conv2d(in_channels=input_shape,\n",
            "                  out_channels=hidden_units,\n",
            "                  kernel_size=3,\n",
            "                  stride=1,\n",
            "                  padding=1),\n",
            "        nn.ReLU(),\n",
            "        nn.Conv2d(in_channels=hidden_units,\n",
            "                  out_channels=hidden_units,\n",
            "                  kernel_size=3,\n",
            "                  stride=1,\n",
            "                  padding=1),\n",
            "        nn.ReLU(),\n",
            "        nn.MaxPool2d(kernel_size=2,\n",
            "                     stride=2)\n",
            "    )\n",
            "    self.conv_block_2 = nn.Sequential(\n",
            "        nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
            "        nn.ReLU(),\n",
            "        nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
            "        nn.ReLU(),\n",
            "        nn.MaxPool2d(2)\n",
            "    )\n",
            "    self.classifier = nn.Sequential(\n",
            "        nn.Flatten(),\n",
            "        # Where did this in_features shape come from?\n",
            "        # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
            "        nn.Linear(in_features=hidden_units*56*56, #*13*13,  # =flattened_size\n",
            "                  out_features=output_shape)\n",
            "    )\n",
            "\n",
            "  def forward(self, x: torch.Tensor):\n",
            "    x = self.conv_block_1(x)\n",
            "    x = self.conv_block_2(x)\n",
            "    x = self.classifier(x)\n",
            "    return x\n",
            "    # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # <- leverage the benefits of operator fusion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class TinyVGG2(nn.Module):\n",
        "  \"\"\"Creates the TinyVGG architecture.\n",
        "\n",
        "  Replicates the TinyVGG architecture from the CNN explainer website in PyTorch.\n",
        "  See the original architecture here: https://poloclub.github.io/cnn-explainer/\n",
        "\n",
        "  Args:\n",
        "    input_shape: An integer indicating number of input channels.\n",
        "    hidden_units: An integer indicating number of hidden units between layers.\n",
        "    output_shape: An integer indicating number of output units.\n",
        "  \"\"\"\n",
        "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
        "    super().__init__()\n",
        "    self.conv_block_1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=input_shape,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,\n",
        "                     stride=2)\n",
        "    )\n",
        "    self.conv_block_2 = nn.Sequential(\n",
        "        nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2)\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        # Where did this in_features shape come from?\n",
        "        # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
        "        nn.Linear(in_features=hidden_units*56*56, #*13*13,\n",
        "                  out_features=output_shape)\n",
        "    )\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    x = self.conv_block_1(x)\n",
        "    x = self.conv_block_2(x)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "3bnGACeUjI3i",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114686465,
          "user_tz": 360,
          "elapsed": 5,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        }
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model_4 = TinyVGG2(input_shape=3,  # number of color channels (3 for RGB)\n",
        "                   hidden_units=10,\n",
        "                   output_shape=len(class_names)).to(device)\n",
        "model_4"
      ],
      "metadata": {
        "id": "tFBY-dGtOjm8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114686474,
          "user_tz": 360,
          "elapsed": 11,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "b519f15c-345f-4a4b-ab03-8a198700d0a9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TinyVGG2(\n",
              "  (conv_block_1): Sequential(\n",
              "    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv_block_2): Sequential(\n",
              "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=31360, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now instead of coding the TinyVGG model from scratch every time, we can import it using:\n",
        "\n",
        "import torch\n",
        "# Import model_builder.py\n",
        "from going_modular import model_builder\n",
        "\n",
        "# Create an instance of TinyVGG - Instantiate an instance of the model from the \"model_builder.py\" script\n",
        "torch.manual_seed(42)\n",
        "model_5 = model_builder.TinyVGG(input_shape=3,  # number of color channels (3 for RGB)\n",
        "                                hidden_units=20,\n",
        "                                output_shape=len(class_names)).to(device)\n",
        "model_5"
      ],
      "metadata": {
        "id": "U_YkwRW_NbjY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114686484,
          "user_tz": 360,
          "elapsed": 9,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "31d32090-9b43-48f7-8265-eb087297772d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TinyVGG(\n",
              "  (conv_block_1): Sequential(\n",
              "    (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv_block_2): Sequential(\n",
              "    (0): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=62720, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# engine.py\n",
        "\n",
        "%%writefile going_modular/engine.py\n",
        "\"\"\"\n",
        "Contains functions for training and testing a PyTorch model.\n",
        "\"\"\"\n",
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device) -> Tuple[float, float]:\n",
        "  \"\"\"Trains a PyTorch model for a single epoch.\n",
        "\n",
        "  Turns a target PyTorch model to training mode and then\n",
        "  runs through all of the required training steps (forward\n",
        "  pass, loss calculation, optimizer step).\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be trained.\n",
        "    dataloader: A DataLoader instance for the model to be trained on.\n",
        "    loss_fn: A PyTorch loss function to minimize.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A tuple of training loss and training accuracy metrics.\n",
        "    In the form (train_loss, train_accuracy). For example:\n",
        "\n",
        "    (0.1112, 0.8743)\n",
        "  \"\"\"\n",
        "  # Put model in train mode\n",
        "  model.train()\n",
        "\n",
        "  # Set up train loss and accuracy values\n",
        "  train_loss, train_acc = 0, 0\n",
        "\n",
        "  # Loop through data loader data batches\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # Send data to target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    # 1. Forward pass\n",
        "    y_pred = model(X)\n",
        "\n",
        "    # 2. Calculate and accumulate loss\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    # 3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. Loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate and accumulate accuracy metric across all batches\n",
        "    y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "    train_acc += (y_pred_class == y).sum().item() / len(y_pred)\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  train_loss /= len(dataloader)\n",
        "  train_acc /= len(dataloader)\n",
        "  return train_loss, train_acc\n",
        "\n",
        "\n",
        "def test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              device: torch.device) -> Tuple[float, float]:\n",
        "  \"\"\"Tests a PyTorch model for a single epoch.\n",
        "\n",
        "  Turns a target PyTorch model to \"eval\" mode and then performs\n",
        "  a forward pass on a testing dataset.\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be tested.\n",
        "    dataloader: A DataLoader instance for the model to be tested on.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A tuple of testing loss and testing accuracy metrics.\n",
        "    In the form (test_loss, test_accuracy). For example:\n",
        "\n",
        "    (0.0223, 0.8985)\n",
        "  \"\"\"\n",
        "  # Put model in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  # Set up test loss and accuracy values\n",
        "  test_loss, test_acc = 0, 0\n",
        "\n",
        "  # Turn on inference context manager\n",
        "  with torch.inference_mode():\n",
        "    # Loop through DataLoader batches\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "      # Send data to target device\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # 1. Forward pass\n",
        "      test_pred_logits = model(X)\n",
        "\n",
        "      # 2. Calculate and accumulate loss\n",
        "      loss = loss_fn(test_pred_logits, y)\n",
        "      test_loss += loss.item()\n",
        "\n",
        "      # Calculate and accumulate accuracy\n",
        "      test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "      test_acc += ((test_pred_labels == y).sum().item() / len(test_pred_labels))\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  test_loss /= len(dataloader)\n",
        "  test_acc /= len(dataloader)\n",
        "  return test_loss, test_acc\n",
        "\n",
        "\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device) -> Dict[str, List]:\n",
        "  \"\"\"Trains and tests a PyTorch model.\n",
        "\n",
        "  Sends a target PyTorch model through train_step() and test_step()\n",
        "  functions for a number of epochs, training and testing the model\n",
        "  in the same epoch loop.\n",
        "\n",
        "  Calculates, prints and stores evaluation metrics throughout.\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be trained and tested.\n",
        "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
        "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
        "    epochs: An integer indicating how many epochs to train for.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A dictionary of training and testing loss as well as training and\n",
        "    testing accuracy metrics. Each metric has a value in a list for\n",
        "    each epoch.\n",
        "    In the form: {train_loss: [...],\n",
        "                  train_acc: [...],\n",
        "                  test_loss: [...],\n",
        "                  test_acc: [...]}\n",
        "    For example if training for epochs=2:\n",
        "                 {train_loss: [2.0616, 1.0537],\n",
        "                  train_acc: [0.3945, 0.3945],\n",
        "                  test_loss: [1.2641, 1.5706],\n",
        "                  test_acc: [0.3400, 0.2973]}\n",
        "  \"\"\"\n",
        "  # Create empty results dictionary\n",
        "  results = {\"train_loss\": [],\n",
        "      \"train_acc\": [],\n",
        "      \"test_loss\": [],\n",
        "      \"test_acc\": []\n",
        "  }\n",
        "\n",
        "  # Loop through training and testing steps for a number of epochs\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "    train_loss, train_acc = train_step(model=model,\n",
        "                                       dataloader=train_dataloader,\n",
        "                                       loss_fn=loss_fn,\n",
        "                                       optimizer=optimizer,\n",
        "                                       device=device)\n",
        "    test_loss, test_acc = test_step(model=model,\n",
        "                                    dataloader=test_dataloader,\n",
        "                                    loss_fn=loss_fn,\n",
        "                                    device=device)\n",
        "\n",
        "    # Print out what's happening\n",
        "    print(\n",
        "        f\"Epoch: {epoch+1} | \"\n",
        "        f\"train_loss: {train_loss:.4f} | \"\n",
        "        f\"train_acc: {train_acc:.4f} | \"\n",
        "        f\"test_loss: {test_loss:.4f} | \"\n",
        "        f\"test_acc: {test_acc:.4f}\"\n",
        "    )\n",
        "\n",
        "    # Update results dictionary\n",
        "    results[\"train_loss\"].append(train_loss)\n",
        "    results[\"train_acc\"].append(train_acc)\n",
        "    results[\"test_loss\"].append(test_loss)\n",
        "    results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "  # Return the filled results at the end of the epochs\n",
        "  return results"
      ],
      "metadata": {
        "id": "dqiup384Si51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114686519,
          "user_tz": 360,
          "elapsed": 32,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "f2e479f0-b5cb-4769-e539-cf10450be572"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "# Set up loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_5.parameters(), lr=0.001)\n",
        "\n",
        "# Now we've got the engine.py script, we can import functions from it via:\n",
        "\n",
        "from going_modular import engine\n",
        "\n",
        "# Use train() by calling it from engine.py\n",
        "engine.train(model=model_4,\n",
        "             train_dataloader=train_dataloader,\n",
        "             test_dataloader=test_dataloader,\n",
        "             optimizer=optimizer,\n",
        "             loss_fn=loss_fn,\n",
        "             epochs=10,\n",
        "             device=device)"
      ],
      "metadata": {
        "id": "leC4fLt8X_JK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917,
          "referenced_widgets": [
            "f780bc20aae34b13893f922b4c7edeb7",
            "fa6ec758f48043c99043711eea4447ad",
            "5088b9fa268b45b7ac8c717b740c5b04",
            "b1679cb4890f4891ba5955d1748b239e",
            "82d73c53a8fd46049bb2a21b521a1642",
            "fbc30d28b2744fdf8f0c715800ed0b70",
            "336ead9f7e0048e0bac25be4cf0d6130",
            "4da6b44aa3e9475f993a612e33f996f0",
            "a44740075756430a9932df595d48554d",
            "fdc453664ad64968bdeba5bd5f9ff89f",
            "c0371c4d57dd435b9872d603e863c592"
          ]
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114735121,
          "user_tz": 360,
          "elapsed": 48592,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "6f384930-9d2b-4b2a-8403-2a6051e98314"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f780bc20aae34b13893f922b4c7edeb7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 1.0989 | train_acc: 0.3444 | test_loss: 1.0966 | test_acc: 0.4200\n",
            "Epoch: 2 | train_loss: 1.0988 | train_acc: 0.3333 | test_loss: 1.0968 | test_acc: 0.4067\n",
            "Epoch: 3 | train_loss: 1.0988 | train_acc: 0.3467 | test_loss: 1.0966 | test_acc: 0.4267\n",
            "Epoch: 4 | train_loss: 1.0989 | train_acc: 0.3311 | test_loss: 1.0967 | test_acc: 0.4000\n",
            "Epoch: 5 | train_loss: 1.0989 | train_acc: 0.3400 | test_loss: 1.0966 | test_acc: 0.4133\n",
            "Epoch: 6 | train_loss: 1.0988 | train_acc: 0.3378 | test_loss: 1.0968 | test_acc: 0.4000\n",
            "Epoch: 7 | train_loss: 1.0988 | train_acc: 0.3422 | test_loss: 1.0967 | test_acc: 0.4200\n",
            "Epoch: 8 | train_loss: 1.0989 | train_acc: 0.3289 | test_loss: 1.0967 | test_acc: 0.4133\n",
            "Epoch: 9 | train_loss: 1.0988 | train_acc: 0.3356 | test_loss: 1.0969 | test_acc: 0.3867\n",
            "Epoch: 10 | train_loss: 1.0988 | train_acc: 0.3422 | test_loss: 1.0968 | test_acc: 0.4000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train_loss': [1.0988962247636582,\n",
              "  1.0988103932804532,\n",
              "  1.098826891846127,\n",
              "  1.0988528317875332,\n",
              "  1.0988813225428264,\n",
              "  1.098845652209388,\n",
              "  1.0987908103730943,\n",
              "  1.098853169017368,\n",
              "  1.098821231789059,\n",
              "  1.0988305441538493],\n",
              " 'train_acc': [0.34444444444444444,\n",
              "  0.3333333333333333,\n",
              "  0.3466666666666667,\n",
              "  0.33111111111111113,\n",
              "  0.34,\n",
              "  0.3377777777777778,\n",
              "  0.3422222222222222,\n",
              "  0.3288888888888889,\n",
              "  0.33555555555555555,\n",
              "  0.3422222222222222],\n",
              " 'test_loss': [1.0966287493705749,\n",
              "  1.0968089151382445,\n",
              "  1.096646505991618,\n",
              "  1.096662877400716,\n",
              "  1.0965514810880026,\n",
              "  1.0967830801010132,\n",
              "  1.0967257444063823,\n",
              "  1.096721550623576,\n",
              "  1.0968641599019369,\n",
              "  1.0967808556556702],\n",
              " 'test_acc': [0.42,\n",
              "  0.4066666666666667,\n",
              "  0.4266666666666667,\n",
              "  0.4,\n",
              "  0.41333333333333333,\n",
              "  0.4,\n",
              "  0.42,\n",
              "  0.41333333333333333,\n",
              "  0.38666666666666666,\n",
              "  0.4]}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular import engine\n",
        "\n",
        "# Use train() by calling it from engine.py\n",
        "engine.train(model=model_5,\n",
        "             train_dataloader=train_dataloader,\n",
        "             test_dataloader=test_dataloader,\n",
        "             optimizer=optimizer,\n",
        "             loss_fn=loss_fn,\n",
        "             epochs=15,\n",
        "             device=device)"
      ],
      "metadata": {
        "id": "n-c93Q9uumEt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d50153ee7d19455db82966849f0e1a15",
            "da3c777b1418486db2d51a0b58a6b626",
            "c6c229c122e44f5ca35ff65ea9caccfd",
            "be7b48600dec45eeac9e00d7b2d5097b",
            "d429eb2fec8a44cca3a00cc8f0b500e7",
            "9f5c167b45494afda259102a2bf40ed1",
            "d8b4d721eb99487c86ebd51c7169d343",
            "886a33e5e7ec4231ba162e0786aeb694",
            "13b748fa77554c358643aba0ba933452",
            "6f7a98ab9b464242bee321596a167cb3",
            "8c29c10ff7f34e0d91fab8ad2b09af58"
          ]
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114797144,
          "user_tz": 360,
          "elapsed": 62010,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "92527664-a83b-47d3-fb9e-472522d768dc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d50153ee7d19455db82966849f0e1a15"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 1.1023 | train_acc: 0.3267 | test_loss: 1.0952 | test_acc: 0.3600\n",
            "Epoch: 2 | train_loss: 1.0964 | train_acc: 0.3689 | test_loss: 1.0881 | test_acc: 0.4200\n",
            "Epoch: 3 | train_loss: 1.0892 | train_acc: 0.4267 | test_loss: 1.0756 | test_acc: 0.5533\n",
            "Epoch: 4 | train_loss: 1.0648 | train_acc: 0.5133 | test_loss: 1.0297 | test_acc: 0.5667\n",
            "Epoch: 5 | train_loss: 0.9943 | train_acc: 0.5422 | test_loss: 0.9243 | test_acc: 0.5800\n",
            "Epoch: 6 | train_loss: 0.9181 | train_acc: 0.5689 | test_loss: 0.9034 | test_acc: 0.6000\n",
            "Epoch: 7 | train_loss: 0.8885 | train_acc: 0.5889 | test_loss: 0.9071 | test_acc: 0.5200\n",
            "Epoch: 8 | train_loss: 0.8722 | train_acc: 0.5822 | test_loss: 0.8783 | test_acc: 0.6333\n",
            "Epoch: 9 | train_loss: 0.8523 | train_acc: 0.6089 | test_loss: 0.8792 | test_acc: 0.5533\n",
            "Epoch: 10 | train_loss: 0.8183 | train_acc: 0.6200 | test_loss: 0.8547 | test_acc: 0.6400\n",
            "Epoch: 11 | train_loss: 0.8061 | train_acc: 0.6578 | test_loss: 0.8928 | test_acc: 0.5800\n",
            "Epoch: 12 | train_loss: 0.8038 | train_acc: 0.6178 | test_loss: 0.8349 | test_acc: 0.6533\n",
            "Epoch: 13 | train_loss: 0.7777 | train_acc: 0.6689 | test_loss: 0.8560 | test_acc: 0.5933\n",
            "Epoch: 14 | train_loss: 0.7485 | train_acc: 0.6733 | test_loss: 0.8416 | test_acc: 0.6867\n",
            "Epoch: 15 | train_loss: 0.7246 | train_acc: 0.7067 | test_loss: 0.8184 | test_acc: 0.6533\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train_loss': [1.1023410379886627,\n",
              "  1.0964280276828342,\n",
              "  1.0891826815075345,\n",
              "  1.064801227119234,\n",
              "  0.9942789374788602,\n",
              "  0.9181414081570175,\n",
              "  0.8884713373167648,\n",
              "  0.8722028453730875,\n",
              "  0.8523107902084788,\n",
              "  0.8182701658995615,\n",
              "  0.8060944353395866,\n",
              "  0.8038272882687548,\n",
              "  0.7777070136699411,\n",
              "  0.7485020143724977,\n",
              "  0.724616241686874],\n",
              " 'train_acc': [0.32666666666666666,\n",
              "  0.3688888888888889,\n",
              "  0.4266666666666667,\n",
              "  0.5133333333333333,\n",
              "  0.5422222222222223,\n",
              "  0.5688888888888889,\n",
              "  0.5888888888888889,\n",
              "  0.5822222222222222,\n",
              "  0.6088888888888889,\n",
              "  0.62,\n",
              "  0.6577777777777778,\n",
              "  0.6177777777777778,\n",
              "  0.6688888888888889,\n",
              "  0.6733333333333333,\n",
              "  0.7066666666666667],\n",
              " 'test_loss': [1.095241957505544,\n",
              "  1.0881380724906922,\n",
              "  1.0755603071053823,\n",
              "  1.0297041964530944,\n",
              "  0.924322037100792,\n",
              "  0.90342609167099,\n",
              "  0.9071315926189224,\n",
              "  0.8782595588080585,\n",
              "  0.8791625795233995,\n",
              "  0.8547136928730955,\n",
              "  0.8928073043158898,\n",
              "  0.8349447031567494,\n",
              "  0.8559761411013702,\n",
              "  0.8415797053424952,\n",
              "  0.8183885492834573],\n",
              " 'test_acc': [0.36,\n",
              "  0.42,\n",
              "  0.5533333333333333,\n",
              "  0.5666666666666667,\n",
              "  0.58,\n",
              "  0.6,\n",
              "  0.52,\n",
              "  0.6333333333333333,\n",
              "  0.5533333333333333,\n",
              "  0.64,\n",
              "  0.58,\n",
              "  0.6533333333333333,\n",
              "  0.5933333333333334,\n",
              "  0.6866666666666666,\n",
              "  0.6533333333333333]}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utils.py\n",
        "\n",
        "%%writefile going_modular/utils.py\n",
        "\"\"\"\n",
        "Contains various utility functions for PyTorch model training and saving.\n",
        "\"\"\"\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "def save_model(model: torch.nn.Module,\n",
        "               target_dir: str,\n",
        "               model_name: str):\n",
        "  \"\"\"Saves a PyTorch model to a target directory.\n",
        "\n",
        "  Args:\n",
        "    model: A target PyTorch model to save.\n",
        "    target_dir: A directory for saving the model to.\n",
        "    model_name: A filename for the saved model. Should include\n",
        "      either \".pth\" or \".pt\" as the file extension.\n",
        "\n",
        "  Example usage:\n",
        "    save_model(model=model_0,\n",
        "               target_dir=\"models\",\n",
        "               model_name=\"05_going_modular_tingvgg_model.pth\")\n",
        "  \"\"\"\n",
        "  # Create a target directory\n",
        "  target_dir_path = Path(target_dir)\n",
        "  target_dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  # Create model save path\n",
        "  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"  # text to display if assert check fails\n",
        "  model_save_path = target_dir / model_name\n",
        "\n",
        "  # Save the model state_dict()\n",
        "  print(f\"[INFO] Saving model to: {model_save_path}\")\n",
        "  torch.save(obj=model.state_dict(),\n",
        "             f=model_save_path)"
      ],
      "metadata": {
        "id": "GnaZVQytv451",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114797190,
          "user_tz": 360,
          "elapsed": 44,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "1fb58fe8-4e30-4c80-8f80-09f18b5c1235"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls going_modular/\n"
      ],
      "metadata": {
        "id": "XH4B8CqlzH3D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114797266,
          "user_tz": 360,
          "elapsed": 75,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "50e8dea4-b86a-436d-81d5-4ddbe8f856a4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_setup.py  engine.py  get_data.py  model_builder.py  __pycache__  utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_dir = Path(\"saved_models\")\n",
        "model_name = \"model_5.pth\"\n",
        "\n",
        "# Now if we wanted to use our save_model() function we can import it and use it via:\n",
        "\n",
        "from going_modular import utils\n",
        "\n",
        "# Save a model to file\n",
        "utils.save_model(model=model_5,\n",
        "                 target_dir=target_dir,\n",
        "                 model_name=model_name)"
      ],
      "metadata": {
        "id": "GV8nPmEA0FCI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114797297,
          "user_tz": 360,
          "elapsed": 29,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "a924a05a-2651-4d69-eb7c-399fc3385be3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Saving model to: saved_models/model_5.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save a model to file\n",
        "utils.save_model(model=model_4,\n",
        "                 target_dir=target_dir,\n",
        "                 model_name=\"model_4.pth\")"
      ],
      "metadata": {
        "id": "Xe_I2ke3ZREr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114797404,
          "user_tz": 360,
          "elapsed": 99,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "0ecf03de-5880-4cc0-e052-642b96f0d762"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Saving model to: saved_models/model_4.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train0.py\n",
        "\n",
        "%%writefile going_modular/train0.py\n",
        "\"\"\"\n",
        "Trains a PyTorch image classification model using device-agnostic code.\n",
        "\"\"\"\n",
        "import os\n",
        "import torch\n",
        "import data_setup, engine, model_builder, utils\n",
        "\n",
        "from torchvision import transforms\n",
        "from pathlib import Path\n",
        "\n",
        "# Set up hyperparameters\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "HIDDEN_UNITS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Set up directories\n",
        "train_dir = \"data/pizza_steak_sushi/train\"\n",
        "test_dir = \"data/pizza_steak_sushi/test\"\n",
        "\n",
        "# Set up target device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Create transforms\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Create DataLoaders with help from data_setup.py\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=data_transform,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Create model with help from model_builder.py\n",
        "model = model_builder.TinyVGG(\n",
        "    input_shape=3,\n",
        "    hidden_units=HIDDEN_UNITS,\n",
        "    output_shape=len(class_names)\n",
        ").to(device)\n",
        "\n",
        "# Set loss and optimizer\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),\n",
        "                            lr=LEARNING_RATE)\n",
        "#optimizer = torch.optim.Adam(model.parameters(),\n",
        "#                             lr=LEARNING_RATE)\n",
        "\n",
        "# Start training with help from engine.py\n",
        "engine.train(model=model,\n",
        "             train_dataloader=train_dataloader,\n",
        "             test_dataloader=test_dataloader,\n",
        "             loss_fn=loss_fn,\n",
        "             optimizer=optimizer,\n",
        "             epochs=NUM_EPOCHS,\n",
        "             device=device)\n",
        "\n",
        "# Save the model with help from utils.py\n",
        "utils.save_model(\n",
        "    model=model,\n",
        "    target_dir=Path(\"models\"),\n",
        "    model_name=\"05_going_modular_script_mode_tinyvgg_model.pth\"\n",
        ")"
      ],
      "metadata": {
        "id": "dtkohcSK4z5-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114797405,
          "user_tz": 360,
          "elapsed": 55,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "18ee298e-69db-4b16-9dad-cbcceb96a116"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/train0.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can train a PyTorch model by running the following line on the command line:\n",
        "!python going_modular/train0.py"
      ],
      "metadata": {
        "id": "C2yMbaEO9ery",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114815968,
          "user_tz": 360,
          "elapsed": 18578,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "160d6d76-58a4-4b81-8de7-a0c6300a55c7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0% 0/5 [00:00<?, ?it/s]Epoch: 1 | train_loss: 1.1023 | train_acc: 0.3208 | test_loss: 1.1003 | test_acc: 0.2875\n",
            " 20% 1/5 [00:03<00:14,  3.56s/it]Epoch: 2 | train_loss: 1.1011 | train_acc: 0.2958 | test_loss: 1.0975 | test_acc: 0.3500\n",
            " 40% 2/5 [00:06<00:09,  3.12s/it]Epoch: 3 | train_loss: 1.0998 | train_acc: 0.3250 | test_loss: 1.0966 | test_acc: 0.3500\n",
            " 60% 3/5 [00:08<00:05,  2.86s/it]Epoch: 4 | train_loss: 1.0995 | train_acc: 0.3292 | test_loss: 1.1007 | test_acc: 0.3500\n",
            " 80% 4/5 [00:11<00:02,  2.75s/it]Epoch: 5 | train_loss: 1.0993 | train_acc: 0.3521 | test_loss: 1.1027 | test_acc: 0.2875\n",
            "100% 5/5 [00:14<00:00,  2.81s/it]\n",
            "[INFO] Saving model to: models/05_going_modular_script_mode_tinyvgg_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python going_modular/get_data.py"
      ],
      "metadata": {
        "id": "UsI6K41bSea4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114816183,
          "user_tz": 360,
          "elapsed": 213,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "229ea9c0-ef9e-44ac-9cd2-9fd1ea4105e2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/pizza_steak_sushi directory exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls data/pizza_steak_sushi/test/pizza"
      ],
      "metadata": {
        "id": "sp-sZiOSKNrw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114816289,
          "user_tz": 360,
          "elapsed": 102,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "c52782f4-f9ed-4300-fdd0-3416ba06ccee"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1001116.jpg  1618659.jpg  2582289.jpg  3486640.jpg  420409.jpg\t771336.jpg\n",
            "1032754.jpg  1687143.jpg  2782998.jpg  3497151.jpg  441659.jpg\t788315.jpg\n",
            "1067986.jpg  204151.jpg   2901001.jpg  3729167.jpg  44810.jpg\t796922.jpg\n",
            "129666.jpg   2111981.jpg  296426.jpg   3770514.jpg  476421.jpg\t833711.jpg\n",
            "1315645.jpg  2250611.jpg  2997525.jpg  3785667.jpg  482858.jpg\t930553.jpg\n",
            "138961.jpg   2398925.jpg  3174637.jpg  380739.jpg   61656.jpg\t998005.jpg\n",
            "148765.jpg   2549661.jpg  3375083.jpg  416067.jpg   648055.jpg\n",
            "1555015.jpg  2572488.jpg  3376617.jpg  419962.jpg   724290.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train.py with argparse\n",
        "\n",
        "%%writefile train.py\n",
        "\"\"\"\n",
        "Trains a PyTorch image classification model using device-agnostic code.\n",
        "\"\"\"\n",
        "import os\n",
        "import torch\n",
        "import argparse\n",
        "\n",
        "from going_modular import data_setup, engine, model_builder, utils\n",
        "from torchvision import transforms\n",
        "from pathlib import Path\n",
        "\n",
        "# Create a parser\n",
        "parser = argparse.ArgumentParser(description=\"Get some hyperparameters.\")\n",
        "\n",
        "# Create an arg for number of epochs\n",
        "parser.add_argument(\"--num_epochs\",\n",
        "                    default=10,\n",
        "                    type=int,\n",
        "                    help=\"the number of epochs to train for\")\n",
        "\n",
        "# Create an arg for batch size\n",
        "parser.add_argument(\"--batch_size\",\n",
        "                    default=32,\n",
        "                    type=int,\n",
        "                    help=\"number of samples per batch\")\n",
        "\n",
        "# Create an arg for hidden units\n",
        "parser.add_argument(\"--hidden_units\",\n",
        "                    default=10,\n",
        "                    type=int,\n",
        "                    help=\"number of hidden units in hidden layers\")\n",
        "\n",
        "# Create an arg for learning rate\n",
        "parser.add_argument(\"--learning_rate\",\n",
        "                    default=0.001,\n",
        "                    type=float,\n",
        "                    help=\"learning rate to use for model\")\n",
        "\n",
        "# Create an arg for training directory\n",
        "parser.add_argument(\"--train_dir\",\n",
        "                    default=\"data/pizza_steak_sushi/train\",\n",
        "                    type=str,\n",
        "                    help=\"directory file path to training data in standard image classification format\")\n",
        "\n",
        "# Create an arg for test directory\n",
        "parser.add_argument(\"--test_dir\",\n",
        "                    default=\"data/pizza_steak_sushi/test\",\n",
        "                    type=str,\n",
        "                    help=\"directory file path to testing data in standard image classification format\")\n",
        "\n",
        "# Create an arg for model name\n",
        "parser.add_argument(\"--model_name\",\n",
        "                    default=\"model_0.pth\",\n",
        "                    type=str,\n",
        "                    help=\"model name to save\")\n",
        "\n",
        "# Get our arguments from the parser\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Set up hyperparameters\n",
        "NUM_EPOCHS = args.num_epochs\n",
        "BATCH_SIZE = args.batch_size\n",
        "HIDDEN_UNITS = args.hidden_units\n",
        "LEARNING_RATE = args.learning_rate\n",
        "print(f\"[INFO] Training a model for {NUM_EPOCHS} epochs with batch size {BATCH_SIZE}, hidden units {HIDDEN_UNITS} and learning rate {LEARNING_RATE}...\")\n",
        "\n",
        "# Set up directories\n",
        "train_dir = args.train_dir\n",
        "test_dir = args.test_dir\n",
        "print(f\"[INFO] Training data file: {train_dir}\")\n",
        "print(f\"[INFO] Testing data file: {test_dir}\")\n",
        "\n",
        "model_name = args.model_name\n",
        "print(f\"[INFO] Model will be saved as: {model_name}\")\n",
        "\n",
        "# Set up target device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Create transforms\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Create DataLoaders with help from data_setup.py\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=data_transform,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Create model with help from model_builder.py\n",
        "model = model_builder.TinyVGG(\n",
        "    input_shape=3,\n",
        "    hidden_units=HIDDEN_UNITS,\n",
        "    output_shape=len(class_names)\n",
        ").to(device)\n",
        "\n",
        "# Set loss and optimizer\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),\n",
        "                            lr=LEARNING_RATE)\n",
        "#optimizer = torch.optim.Adam(model.parameters(),\n",
        "#                             lr=LEARNING_RATE)\n",
        "\n",
        "# Start training with help from engine.py\n",
        "engine.train(model=model,\n",
        "             train_dataloader=train_dataloader,\n",
        "             test_dataloader=test_dataloader,\n",
        "             loss_fn=loss_fn,\n",
        "             optimizer=optimizer,\n",
        "             epochs=NUM_EPOCHS,\n",
        "             device=device)\n",
        "\n",
        "# Save the model with help from utils.py\n",
        "target_dir = Path(\"saved_models\")\n",
        "\n",
        "utils.save_model(\n",
        "    model=model,\n",
        "    target_dir=target_dir,\n",
        "    model_name=model_name\n",
        ")"
      ],
      "metadata": {
        "id": "caSFN-FkLvm4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114816360,
          "user_tz": 360,
          "elapsed": 65,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "cb1847f0-593c-4974-942d-68dfacead0f6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --num_epochs 15 --batch_size 32 --hidden_units 20 --model_name model_1.pth"
      ],
      "metadata": {
        "id": "FfwoEXHGFOe4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114862942,
          "user_tz": 360,
          "elapsed": 46587,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "be444bba-d810-435e-f457-0963af6cf94d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Training a model for 15 epochs with batch size 32, hidden units 20 and learning rate 0.001...\n",
            "[INFO] Training data file: data/pizza_steak_sushi/train\n",
            "[INFO] Testing data file: data/pizza_steak_sushi/test\n",
            "[INFO] Model will be saved as: model_1.pth\n",
            "  0% 0/15 [00:00<?, ?it/s]Epoch: 1 | train_loss: 1.0987 | train_acc: 0.3438 | test_loss: 1.1017 | test_acc: 0.2875\n",
            "  7% 1/15 [00:03<00:42,  3.05s/it]Epoch: 2 | train_loss: 1.0983 | train_acc: 0.3521 | test_loss: 1.1022 | test_acc: 0.2875\n",
            " 13% 2/15 [00:05<00:36,  2.81s/it]Epoch: 3 | train_loss: 1.1005 | train_acc: 0.3208 | test_loss: 1.0992 | test_acc: 0.2938\n",
            " 20% 3/15 [00:09<00:37,  3.15s/it]Epoch: 4 | train_loss: 1.0991 | train_acc: 0.3229 | test_loss: 1.0977 | test_acc: 0.3625\n",
            " 27% 4/15 [00:11<00:32,  2.98s/it]Epoch: 5 | train_loss: 1.1000 | train_acc: 0.3354 | test_loss: 1.0970 | test_acc: 0.3625\n",
            " 33% 5/15 [00:14<00:28,  2.86s/it]Epoch: 6 | train_loss: 1.0999 | train_acc: 0.3042 | test_loss: 1.0992 | test_acc: 0.3443\n",
            " 40% 6/15 [00:17<00:25,  2.78s/it]Epoch: 7 | train_loss: 1.0998 | train_acc: 0.2979 | test_loss: 1.0974 | test_acc: 0.3625\n",
            " 47% 7/15 [00:20<00:22,  2.77s/it]Epoch: 8 | train_loss: 1.0985 | train_acc: 0.3146 | test_loss: 1.0991 | test_acc: 0.3625\n",
            " 53% 8/15 [00:23<00:21,  3.01s/it]Epoch: 9 | train_loss: 1.0989 | train_acc: 0.2979 | test_loss: 1.0981 | test_acc: 0.3625\n",
            " 60% 9/15 [00:26<00:17,  2.90s/it]Epoch: 10 | train_loss: 1.0995 | train_acc: 0.3042 | test_loss: 1.1029 | test_acc: 0.2875\n",
            " 67% 10/15 [00:28<00:14,  2.80s/it]Epoch: 11 | train_loss: 1.0974 | train_acc: 0.3521 | test_loss: 1.1031 | test_acc: 0.2875\n",
            " 73% 11/15 [00:31<00:11,  2.77s/it]Epoch: 12 | train_loss: 1.0983 | train_acc: 0.3563 | test_loss: 1.1034 | test_acc: 0.2875\n",
            " 80% 12/15 [00:35<00:09,  3.04s/it]Epoch: 13 | train_loss: 1.1004 | train_acc: 0.3208 | test_loss: 1.1008 | test_acc: 0.3517\n",
            " 87% 13/15 [00:37<00:05,  2.93s/it]Epoch: 14 | train_loss: 1.0987 | train_acc: 0.3271 | test_loss: 1.0984 | test_acc: 0.3500\n",
            " 93% 14/15 [00:40<00:02,  2.82s/it]Epoch: 15 | train_loss: 1.0981 | train_acc: 0.3271 | test_loss: 1.0970 | test_acc: 0.3500\n",
            "100% 15/15 [00:43<00:00,  2.87s/it]\n",
            "[INFO] Saving model to: saved_models/model_1.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes too long, too slow with batch_size=64 or 128 and hidden_units=64 or 128\n",
        "!python train.py --num_epochs 3 --batch_size 64 --hidden_units 16 --learning_rate 0.002 --model_name model_2.pth"
      ],
      "metadata": {
        "id": "vi4rQo5STSPJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114875282,
          "user_tz": 360,
          "elapsed": 12339,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "cf243977-0d5c-42dc-f1e3-5ba21f463654"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Training a model for 3 epochs with batch size 64, hidden units 16 and learning rate 0.002...\n",
            "[INFO] Training data file: data/pizza_steak_sushi/train\n",
            "[INFO] Testing data file: data/pizza_steak_sushi/test\n",
            "[INFO] Model will be saved as: model_2.pth\n",
            "  0% 0/3 [00:00<?, ?it/s]Epoch: 1 | train_loss: 1.0977 | train_acc: 0.3613 | test_loss: 1.1148 | test_acc: 0.2396\n",
            " 33% 1/3 [00:02<00:05,  2.94s/it]Epoch: 2 | train_loss: 1.1067 | train_acc: 0.3008 | test_loss: 1.0910 | test_acc: 0.4583\n",
            " 67% 2/3 [00:05<00:02,  2.74s/it]Epoch: 3 | train_loss: 1.1034 | train_acc: 0.2930 | test_loss: 1.0989 | test_acc: 0.3021\n",
            "100% 3/3 [00:08<00:00,  2.71s/it]\n",
            "[INFO] Saving model to: saved_models/model_2.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls models\n",
        "!ls saved_models/"
      ],
      "metadata": {
        "id": "1x4dgQTJhcBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114875503,
          "user_tz": 360,
          "elapsed": 217,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "a2f4f332-db2e-44f3-b59c-1787251c979c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05_going_modular_script_mode_tinyvgg_model.pth\n",
            "model_1.pth  model_2.pth  model_4.pth  model_5.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile predict.py\n",
        "\"\"\"\n",
        "Makes predictions with a trained PyTorch model and saves the results to file.\n",
        "\"\"\"\n",
        "import torch\n",
        "import torchvision\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from going_modular import model_builder\n",
        "from torchvision import transforms\n",
        "\n",
        "# Create a parser\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "# Create an arg for model path\n",
        "parser.add_argument(\"--model_path\",\n",
        "                    default=\"models/05_going_modular_script_mode_tinyvgg_model.pth\",\n",
        "                    type=str,\n",
        "                    help=\"filepath of model to use for prediction\")\n",
        "\n",
        "# Create an arg for image path\n",
        "parser.add_argument(\"--image_path\",\n",
        "                    default=\"data/pizza_steak_sushi/train/pizza/12301.jpg\",\n",
        "                    type=str,\n",
        "                    help=\"filepath of image to predict on\")\n",
        "\n",
        "# Create an arg for transform type\n",
        "parser.add_argument(\"--transform\",\n",
        "                    default=\"no\",\n",
        "                    type=str,\n",
        "                    help=\"yes to transform using horizontal flip, no is default\")\n",
        "\n",
        "# Create an arg for hidden units\n",
        "parser.add_argument(\"--hidden_units\",\n",
        "                    default=10,\n",
        "                    type=int,\n",
        "                    help=\"number of hidden units used by model\")\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "model_path = args.model_path\n",
        "image_path = args.image_path\n",
        "transform = args.transform\n",
        "hidden_units = args.hidden_units\n",
        "\n",
        "print(f\"[INFO] Predicting on {image_path} with {model_path}\")\n",
        "\n",
        "# Set up class names\n",
        "class_names = [\"pizza\", \"steak\", \"sushi\"]\n",
        "\n",
        "# Set up device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Need to use same hyperparameters as saved model\n",
        "model = model_builder.TinyVGG(input_shape=3,\n",
        "                              hidden_units=hidden_units,\n",
        "                              output_shape=3).to(device)  # len(class_names) = 3\n",
        "\n",
        "# Load in the saved model state dictionary from file\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# 1. Load in an image and convert tensor values to float32\n",
        "target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)\n",
        "\n",
        "# 2. Divide the image pixel values by 255 to get them between 0 and 1\n",
        "target_image /= 255\n",
        "\n",
        "# 3. Transform if necessary\n",
        "if transform == \"yes\":\n",
        "  data_transform_flip = transforms.Compose([\n",
        "    transforms.Resize((224,224)),#64,64)),\n",
        "    # Flip images randomly on the horizontal\n",
        "    transforms.RandomHorizontalFlip(p=0.5),  # p = probability of flip, 0.5 = 50% chance\n",
        "    #transforms.ToTensor()\n",
        "  ])\n",
        "  target_image = data_transform_flip(target_image)\n",
        "  print(\"Using data_transform_flip\")\n",
        "else:  # Resize the image to be the same size as the model\n",
        "  data_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),#64,64)),\n",
        "    #transforms.ToTensor()\n",
        "  ])\n",
        "  target_image = data_transform(target_image)\n",
        "\n",
        "# 4. Make sure model is on target device\n",
        "model.to(device)\n",
        "\n",
        "# 5. Turn on model evaluation and inference modes\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "\n",
        "  # Add an extra dimension to image\n",
        "  target_image = target_image.unsqueeze(dim=0)\n",
        "  # Make a prediction on image with an extra dimension and send it to the target device\n",
        "  target_image_pred = model(target_image.to(device))\n",
        "\n",
        "# 6. Convert logits to probabilities\n",
        "target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
        "\n",
        "# 7. Convert probs to label\n",
        "target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
        "\n",
        "# 8. Plot the image alongside the prediction and prediction probability\n",
        "plt.imshow(target_image.squeeze().permute(1, 2, 0))  # make sure it's right size for matplotlib\n",
        "if class_names:\n",
        "  title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n",
        "else:\n",
        "  title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n",
        "plt.title(title)\n",
        "plt.axis(False);\n",
        "\n",
        "print(f\"[INFO] Prediction label: {class_names[target_image_pred_label]}, prediction probability: {target_image_pred_probs.max():.3f}\")"
      ],
      "metadata": {
        "id": "Le6hUvDcfQcA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114875570,
          "user_tz": 360,
          "elapsed": 63,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "54beb65d-11dd-4a32-f85a-4d654af26686"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing predict.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls saved_models/"
      ],
      "metadata": {
        "id": "iZsxja9KqtuL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114875675,
          "user_tz": 360,
          "elapsed": 103,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "ef71ed3d-ecd9-4324-9ed2-f738961b603b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1.pth  model_2.pth  model_4.pth  model_5.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python predict.py --image_path data/pizza_steak_sushi/train/pizza/300869.jpg --model_path saved_models/model_5.pth --hidden_units 20 --transform yes\n",
        "!python predict.py --image_path data/pizza_steak_sushi/train/pizza/300869.jpg --model_path saved_models/model_5.pth --hidden_units 20"
      ],
      "metadata": {
        "id": "EGJuSyGEgDSq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114885168,
          "user_tz": 360,
          "elapsed": 9488,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "a6dab961-f87f-4bfc-d227-a3b142a46355"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Predicting on data/pizza_steak_sushi/train/pizza/300869.jpg with saved_models/model_5.pth\n",
            "Using data_transform_flip\n",
            "[INFO] Prediction label: steak, prediction probability: 0.584\n",
            "[INFO] Predicting on data/pizza_steak_sushi/train/pizza/300869.jpg with saved_models/model_5.pth\n",
            "[INFO] Prediction label: steak, prediction probability: 0.597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python predict.py --image_path data/pizza_steak_sushi/test/pizza/61656.jpg --model_path saved_models/model_5.pth --hidden_units 20\n",
        "!python predict.py --image_path data/pizza_steak_sushi/test/pizza/61656.jpg --model_path saved_models/model_5.pth --hidden_units 20 --transform yes"
      ],
      "metadata": {
        "id": "vsfcCtvPtZSE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114894400,
          "user_tz": 360,
          "elapsed": 9230,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "14b91f7c-d47a-436a-976c-b042a616ca59"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Predicting on data/pizza_steak_sushi/test/pizza/61656.jpg with saved_models/model_5.pth\n",
            "[INFO] Prediction label: sushi, prediction probability: 0.679\n",
            "[INFO] Predicting on data/pizza_steak_sushi/test/pizza/61656.jpg with saved_models/model_5.pth\n",
            "Using data_transform_flip\n",
            "[INFO] Prediction label: sushi, prediction probability: 0.679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get_custom_data.py\n",
        "\n",
        "%%writefile going_modular/get_custom_data.py\n",
        "\"\"\"\n",
        "Contains functionality to download custom images from GitHub\n",
        "\"\"\"\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "data_path = Path(\"data\")\n",
        "\n",
        "# Get multiple custom images\n",
        "\"\"\" to get raw address:  right click jpeg file name in main repository view, select copy link address, paste into browser and enter\n",
        "    then right click on image, select copy link address = https://github.com/lanehale/pytorch-deep-learning/blob/main/cheese-pizza.jpeg?raw=true\n",
        "    paste into browser and enter to get url format below\n",
        "\"\"\"\n",
        "urls = [\n",
        "    \"https://raw.githubusercontent.com/lanehale/pytorch-deep-learning/refs/heads/main/custom_images/cheese-pizza.jpeg\",\n",
        "    \"https://raw.githubusercontent.com/lanehale/pytorch-deep-learning/refs/heads/main/custom_images/pizza-slice.jpeg\",\n",
        "    \"https://raw.githubusercontent.com/lanehale/pytorch-deep-learning/refs/heads/main/custom_images/pizza-slice2.jpeg\",\n",
        "    \"https://raw.githubusercontent.com/lanehale/pytorch-deep-learning/refs/heads/main/custom_images/pizza-sliced.jpeg\",\n",
        "    \"https://raw.githubusercontent.com/lanehale/pytorch-deep-learning/refs/heads/main/custom_images/pizza-sliced2.jpeg\",\n",
        "    \"https://raw.githubusercontent.com/lanehale/pytorch-deep-learning/refs/heads/main/custom_images/pizza-partial-view.jpeg\",\n",
        "    \"https://raw.githubusercontent.com/lanehale/pytorch-deep-learning/refs/heads/main/custom_images/pizza-partial-view2.jpeg\",\n",
        "    \"https://raw.githubusercontent.com/lanehale/pytorch-deep-learning/refs/heads/main/custom_images/pizza-side-view.jpeg\"\n",
        "]\n",
        "\n",
        "filenames = [\n",
        "    \"cheese-pizza.jpeg\",\n",
        "    \"pizza-slice.jpeg\",\n",
        "    \"pizza-slice2.jpeg\",\n",
        "    \"pizza-sliced.jpeg\",\n",
        "    \"pizza-sliced2.jpeg\",\n",
        "    \"pizza-partial-view.jpeg\",\n",
        "    \"pizza-partial-view2.jpeg\",\n",
        "    \"pizza-side-view.jpeg\"\n",
        "]\n",
        "\n",
        "if len(urls) != len(filenames):\n",
        "  raise ValueError(\"The number of URLs and filenames must be the same.\")\n",
        "\n",
        "# Download the images if they don't already exist\n",
        "if (data_path / \"cheese-pizza.jpeg\").is_file():\n",
        "  print(f\"Custom images already exist, skipping download.\")\n",
        "else:\n",
        "  for i, url in enumerate(urls):\n",
        "    try:\n",
        "      response = requests.get(url)\n",
        "      response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "\n",
        "      custom_image_path = data_path / filenames[i]\n",
        "\n",
        "      with open(custom_image_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "      print(f\"DownLoading {custom_image_path}...\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "      print(f\"Error downloading {url}: {e}\")\n",
        "    except Exception as e:\n",
        "      print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "id": "a8PY83Z0uMIr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114894478,
          "user_tz": 360,
          "elapsed": 75,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "51b78922-c6b9-4901-e4c0-bb18f918a264"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/get_custom_data.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get custom images\n",
        "!python going_modular/get_custom_data.py"
      ],
      "metadata": {
        "id": "Nt1JW7qRxxHN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114897524,
          "user_tz": 360,
          "elapsed": 3052,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "e0f04525-42d5-46e2-ce9d-31b812563581"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DownLoading data/cheese-pizza.jpeg...\n",
            "DownLoading data/pizza-slice.jpeg...\n",
            "DownLoading data/pizza-slice2.jpeg...\n",
            "DownLoading data/pizza-sliced.jpeg...\n",
            "DownLoading data/pizza-sliced2.jpeg...\n",
            "DownLoading data/pizza-partial-view.jpeg...\n",
            "DownLoading data/pizza-partial-view2.jpeg...\n",
            "DownLoading data/pizza-side-view.jpeg...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls data"
      ],
      "metadata": {
        "id": "MZQ0l4IQvfnL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114897644,
          "user_tz": 360,
          "elapsed": 118,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "3ef5a7ad-863c-45eb-a019-c63a9a8dc913"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cheese-pizza.jpeg\t  pizza-side-view.jpeg\tpizza-sliced.jpeg\n",
            "pizza-partial-view2.jpeg  pizza-slice2.jpeg\tpizza-slice.jpeg\n",
            "pizza-partial-view.jpeg   pizza-sliced2.jpeg\tpizza_steak_sushi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python predict.py --image_path data/cheese-pizza.jpeg --model_path saved_models/model_5.pth --hidden_units 20"
      ],
      "metadata": {
        "id": "WY7wDIfjvaKA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114902471,
          "user_tz": 360,
          "elapsed": 4826,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "cd0893f4-3c46-4d48-926f-316243beb914"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Predicting on data/cheese-pizza.jpeg with saved_models/model_5.pth\n",
            "[INFO] Prediction label: pizza, prediction probability: 0.579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python predict.py --image_path data/pizza-sliced.jpeg --model_path saved_models/model_5.pth --hidden_units 20"
      ],
      "metadata": {
        "id": "OKcJYPala7Ik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114906685,
          "user_tz": 360,
          "elapsed": 4212,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "a8a5c4af-7818-4435-bb16-fd8fba3910a7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Predicting on data/pizza-sliced.jpeg with saved_models/model_5.pth\n",
            "[INFO] Prediction label: sushi, prediction probability: 0.631\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python predict.py --image_path data/pizza-partial-view.jpeg --model_path saved_models/model_5.pth --hidden_units 20"
      ],
      "metadata": {
        "id": "SmSjLcnwbBD6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1750114910910,
          "user_tz": 360,
          "elapsed": 4222,
          "user": {
            "displayName": "Lane Hale",
            "userId": "11862511056076326919"
          }
        },
        "outputId": "f3315877-8941-48b3-92f7-553a250336ac"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Predicting on data/pizza-partial-view.jpeg with saved_models/model_5.pth\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..1.0000001].\n",
            "[INFO] Prediction label: pizza, prediction probability: 0.714\n"
          ]
        }
      ]
    }
  ]
}
